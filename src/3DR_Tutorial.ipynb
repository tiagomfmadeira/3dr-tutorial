{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obGT-YQP1b4F"
   },
   "source": [
    "# 3D Reconstruction Tutorial: Data Processing, Surface Reconstruction, and Texture Mapping\n",
    "\n",
    "Tiago Madeira, Lucal Dal'Col, Miguel Oliveira, and Paulo Dias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this tutorial, we will walk through a complete 3D reconstruction pipeline.\\\n",
    "Press **<shift + enter>** to execute the current cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGsn-tzm1-Ft"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "import pyfqmr\n",
    "import pickle\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom modules\n",
    "from texture_mapping import compute_texture, get_image_scores\n",
    "from utils import get_cameras_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's define the input arguments.\n",
    "\n",
    "For logistical reasons, we have provided cached data in the **<./cache>** folder, instead of the original data source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCjwvvJt8P93"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'input_type': 'e57',\n",
    "    'input_path': './024VARLab_web3d.e57',\n",
    "    'cache_path': './cache',\n",
    "    'output_path': './output',\n",
    "    'voxel_size': 0.01,\n",
    "    'max_decimation_error': 0.0001,\n",
    "    'image_selection_algorithm': 'score',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmhsGgK__Fv5"
   },
   "source": [
    "    -type', '--input_type':\n",
    "                  Type of input files.\n",
    "                      'e57' -  E57 file containing laser scan, RGB camera captures and sensors parameters.\n",
    "                      'conf' - Config file containing path to RGB and depth images along with camera parameters.\n",
    "\n",
    "    -i', '--input_path':\n",
    "                  Path to the input file.\n",
    "                  \n",
    "    -cache, --cache_path:\n",
    "                  Path for cache directory.\n",
    "\n",
    "    -o , --output_path:\n",
    "                  Path to the output directory.\n",
    "                  \n",
    "    -voxel, --voxel_size:    (default: 0.008)\n",
    "                  Size for finest level octree cells in meters.\n",
    "                  \n",
    "    -mde, --max_decimation_error:    (default: 0.00001)\n",
    "                  Acceptable error for Fast Quadric Mesh Simplification.\n",
    "                  \n",
    "    -isa, --image_selection_algorithm:    (default: area)\n",
    "                  Algorithm for image selection in texture mapping.\n",
    "                      'random'  - Random image selection.\n",
    "                      'area'  - Image with the largest projection available for triangle.\n",
    "                      'score' - Image with the highest score (preserve continuity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Load the data to memory.\n",
    "\n",
    "Cached data is used to speed up the tutorial, but all the loading functionality is provided in utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJ2quh8GCfXb"
   },
   "outputs": [],
   "source": [
    "# Extract filename from the input path\n",
    "file_out_name = os.path.basename(args['input_path']).split('.')[0]\n",
    "print(file_out_name, 'dataset')\n",
    "\n",
    "# Load the camera data\n",
    "if args['cache_path'] and os.path.isfile(args['cache_path'] + '/camera_data.pickle'):\n",
    "    with open(args['cache_path'] + '/camera_data.pickle', 'rb') as handle:\n",
    "        cameras_data = pickle.load(handle)\n",
    "else:\n",
    "    cameras_data = get_cameras_data(args['input_path'], args['input_type'])\n",
    "\n",
    "\n",
    "print('\\nNumber of captures = ', len(cameras_data.keys()))\n",
    "print('\\nData contained in each capture: ')\n",
    "print(list(cameras_data[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's explore this data to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through each capture\n",
    "for capture_id, capture_data in cameras_data.items():\n",
    "    print(f\"Visualizing capture {capture_id + 1} / {len(cameras_data.items())}\")\n",
    "    \n",
    "    # Extract imageBGR and scan_depth_image from the capture\n",
    "    imageBGR = capture_data.get('imageBGR')\n",
    "    depth_image = np.copy(capture_data.get('scan_depth_image'))\n",
    "    \n",
    "    # Convert BGR to RGB for proper display with matplotlib\n",
    "    imageRGB = cv2.cvtColor(imageBGR, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Plot imageBGR\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(imageRGB)\n",
    "    plt.title(f'Capture {capture_id} - RGB Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plot scan_depth_images\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(depth_image, cmap='gray')\n",
    "    plt.title(f'Capture {capture_id} - Depth Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display the images\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy6MQiT28R2a"
   },
   "source": [
    "---\n",
    "\n",
    "We will now define a function to integrate our depth and colour into a common volumentric data structure.\n",
    "\n",
    "The `create_tsdf_from_rgbd` function generates a Truncated Signed Distance Function (TSDF) volume from RGB-D images.\n",
    "\n",
    "We loop over each camera's data and integrate its RGB-D image into the TSDF volume using the camera pose given by the intrinsic and extrinsic camera parameters.\n",
    "\n",
    "The TSDF represents the 3D scene as a volumetric grid. Each voxel (a 3D pixel) in this grid holds information about the distance to the nearest surface, allowing for efficient fusion of multiple 3D scans.\n",
    "\n",
    "The signed distance function ùëÜ(ùëù) at point ùëù is:\n",
    "\n",
    "ùëÜ(ùëù) > 0 (outside the object),\\\n",
    "ùëÜ(ùëù) < 0 (inside the object),\\\n",
    "ùëÜ(ùëù) = 0 (on the surface).\n",
    "\n",
    "ùëÜ(ùëù) is constructed using captures from different perspectives, where each pixel‚Äôs depth value provides a point on a surface in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oomzLpJ4Qgb"
   },
   "outputs": [],
   "source": [
    "# Function to create a Truncated Signed Distance Function (TSDF) volume from RGB-D images\n",
    "def create_tsdf_from_rgbd(cameras_data, voxel_size, visualize = False):\n",
    "    # Initialize a Scalable TSDF Volume\n",
    "    volume = o3d.pipelines.integration.ScalableTSDFVolume(\n",
    "        voxel_length= voxel_size,\n",
    "        sdf_trunc= 10 * voxel_size,\n",
    "        color_type= o3d.pipelines.integration.TSDFVolumeColorType.RGB8)\n",
    "\n",
    "    print(\"\\nIntegrating RGBD data in TSDF...\")\n",
    "    for i, camera_id in enumerate(tqdm(cameras_data)):\n",
    "            \n",
    "        camera = cameras_data[camera_id]\n",
    "        image = o3d.geometry.Image(cv2.cvtColor(camera['imageBGR'], cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Create RGBD image from color and depth\n",
    "        rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "            image,\n",
    "            o3d.geometry.Image(camera['scan_depth_image'] * 1000),\n",
    "            depth_trunc=100.0,\n",
    "            convert_rgb_to_intensity=False)\n",
    "\n",
    "        # Get intrinsic and extrinsic camera parameters\n",
    "        K = camera['intrinsics']\n",
    "        width = camera['width']\n",
    "        height = camera['height']\n",
    "        o3d_intrinsics = o3d.camera.PinholeCameraIntrinsic(width, height, K[0, 0], K[1, 1], K[0, 2], K[1, 2])\n",
    "\n",
    "        # Integrate RGBD image into TSDF volume\n",
    "        volume.integrate(rgbd, o3d_intrinsics, np.linalg.inv(camera['extrinsics']))\n",
    "        \n",
    "        ############################################\n",
    "        # Visualization\n",
    "        if visualize:\n",
    "            pcd = volume.extract_point_cloud()\n",
    "            \n",
    "            def create_cube(color, size):\n",
    "                cube_mesh = o3d.geometry.TriangleMesh.create_box(width=size, height=size, depth=size)\n",
    "                color = np.clip(np.asarray(color), 0, 1)\n",
    "                cube_mesh.paint_uniform_color(color)\n",
    "                return cube_mesh\n",
    "\n",
    "            voxels = o3d.geometry.TriangleMesh()\n",
    "            colors = np.asarray(pcd.colors)\n",
    "            \n",
    "            for i in range(len(pcd.points)):\n",
    "                point = pcd.points[i]\n",
    "                color = colors[i]\n",
    "                cube = create_cube(color, voxel_size * 0.90)\n",
    "                cube.translate(point)\n",
    "                voxels += cube\n",
    "                \n",
    "            o3d.visualization.draw_geometries([voxels], window_name=f\"TSDF representation - Capture {camera_id}\")\n",
    "        \n",
    "        ############################################\n",
    "\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's visualize the integration of each scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tpc424liDTlA"
   },
   "outputs": [],
   "source": [
    "# Create TSDF volume from RGB-D images with larger voxel size for visualization\n",
    "create_tsdf_from_rgbd(cameras_data, voxel_size=0.1, visualize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['voxel_size'] = 0.005\n",
    "\n",
    "# Create TSDF volume from RGB-D images\n",
    "tsdf = create_tsdf_from_rgbd(cameras_data, voxel_size=args['voxel_size'], visualize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Mesh extraction from the continuous function. The Marching Cubes algorithm generates a mesh by identifying and connecting surfaces in the 3D volume that correspond to the boundary of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJaUmzttC8B_"
   },
   "outputs": [],
   "source": [
    "# Extract mesh from TSDF volume using Marching Cubes algorithm\n",
    "print(\"\\nExtracting mesh...\")\n",
    "mesh = tsdf.extract_triangle_mesh()\n",
    "# mesh.compute_vertex_normals()\n",
    "\n",
    "o3d.visualization.draw_geometries([mesh], window_name=\"Mesh before decimation\")\n",
    "print(\"Visualizing the mesh before decimation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth the mesh surface (used for noisy data)\n",
    "print(\"\\nSmoothing mesh...\")\n",
    "# mesh = mesh.filter_smooth_laplacian(number_of_iterations=25, lambda_filter=0.5)\n",
    "mesh = mesh.filter_smooth_taubin(number_of_iterations=10, lambda_filter=0.5, mu=-0.53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tkVt76wDham"
   },
   "source": [
    "---\n",
    "Mesh decimation is the process of simplifying the 3D mesh to reduce its complexity, while keeping the most important geometric features intact. This is crucial when working with high-resolution models, as it makes processing faster and reduces memory usage.\n",
    "\n",
    "In this step, we define a maximum allowable error for decimation using the **max_decimation_error** parameter. A smaller error value results in a higher-fidelity mesh, while a larger value allows more aggressive simplification.\n",
    "\n",
    "Here, we use Quadric Mesh Simplification to reduce the mesh complexity. This algorithm works by iteratively collapsing edges while minimizing geometric distortion.\n",
    "\n",
    "\n",
    "**Mesh Decimation**: Simplifies the mesh to reduce its complexity while preserving important geometric features.\n",
    "\n",
    "**Mesh Cleanup**: The mesh is cleaned to remove errors such as duplicate vertices, triangles, and degenerate shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLU_E855Dcon"
   },
   "outputs": [],
   "source": [
    "args['max_decimation_error'] = 0.0001\n",
    "\n",
    "# Decimate the mesh using Quadric Mesh Simplification\n",
    "print(\"\\nDecimating mesh...\")\n",
    "mesh_simplifier = pyfqmr.Simplify()\n",
    "mesh_simplifier.setMesh(np.asarray(mesh.vertices), np.asarray(mesh.triangles))\n",
    "mesh_simplifier.simplify_mesh(target_count=0, update_rate=1, max_iterations=20, aggressiveness=0,\n",
    "                              preserve_border=True, alpha=args['max_decimation_error'], K=0, verbose=True)\n",
    "vertices, faces, face_normals = mesh_simplifier.getMesh()\n",
    "\n",
    "# Recreate simplified mesh\n",
    "final_mesh = o3d.geometry.TriangleMesh()\n",
    "final_mesh.vertices = o3d.utility.Vector3dVector(vertices)\n",
    "final_mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "\n",
    "# Clean up the mesh topology\n",
    "print(\"\\nCleaning up topology...\")\n",
    "final_mesh.remove_duplicated_triangles()\n",
    "final_mesh.remove_degenerate_triangles()\n",
    "final_mesh.remove_duplicated_vertices()\n",
    "final_mesh.remove_non_manifold_edges()\n",
    "final_mesh.remove_unreferenced_vertices()\n",
    "\n",
    "o3d.visualization.draw_geometries([final_mesh], window_name=\"Mesh after decimation\")\n",
    "print(\"Visualizing the final mesh after decimation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Z-buffering helps determine the visibility of different triangles of the mesh from the perspective of each camera. The Z-buffer (or depth buffer) stores the distance from the camera to the nearest object for each pixel, which is crucial for deciding which parts of the mesh are visible when rendering textures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cIDYq1dGGeu"
   },
   "outputs": [],
   "source": [
    "print(\"\\nGenerating data for Z-buffering...\")\n",
    "for camera_idx in tqdm(cameras_data):\n",
    "    camera = cameras_data[camera_idx]\n",
    "    renderer = o3d.visualization.rendering.OffscreenRenderer(camera['width'], camera['height'])\n",
    "    mtl = o3d.visualization.rendering.MaterialRecord()\n",
    "    mtl.base_color = [1.0, 1.0, 1.0, 1.0]  # RGBA\n",
    "    mtl.shader = \"defaultUnlit\"\n",
    "    renderer.scene.add_geometry(\"textured_mesh\", final_mesh, mtl)\n",
    "    K = camera['intrinsics']\n",
    "    cam_matrix = camera['extrinsics']\n",
    "    o3d_intrinsics = o3d.camera.PinholeCameraIntrinsic(camera['width'], camera['height'],\n",
    "                                                        K[0, 0], K[1, 1], K[0, 2], K[1, 2])\n",
    "    renderer.setup_camera(o3d_intrinsics, np.linalg.inv(cam_matrix))\n",
    "\n",
    "    depth_image = np.array(renderer.render_to_depth_image(z_in_view_space=True))\n",
    "\n",
    "    camera['mesh_depth'] = depth_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Texture mapping is the process of applying a 2D image (called a texture) onto a 3D surface (the mesh) to give it detailed appearance or color. \n",
    "Instead of relying on just the mesh's geometry and colour per vertex, texture mapping allows the mesh to display colors, patterns, or surface details based on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vOYbEoTF3Cp"
   },
   "outputs": [],
   "source": [
    "args['image_selection_algorithm'] = 'area'\n",
    "\n",
    "print(\"\\nCreating texture map...\")\n",
    "textured_mesh = compute_texture(final_mesh, cameras_data, args['image_selection_algorithm'], img_filter=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We export the textured 3D mesh into a format that includes an .obj file, a .mtl file, and the associated image files for textures.\n",
    "\n",
    "\n",
    "By exporting the mesh in this format, the textured mesh can be opened and visualized in many 3D software packages (e.g., Blender, Maya), or used in real-time applications (e.g., game engines). The .obj format is a widely supported standard for storing 3D models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99VtxZZfF3Ex"
   },
   "outputs": [],
   "source": [
    "print(\"\\nExporting mesh...\")\n",
    "o3d.io.write_triangle_mesh(args['output_path'] + '/' + file_out_name + '.obj', textured_mesh,\n",
    "                            write_triangle_uvs=True, print_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "At this stage, you can interactively explore your mesh and its texture mapping using the Graphical User Interface (GUI) provided by the visualization tool. This allows you to analyze how textures are applied to the surface of the 3D model.\n",
    "\n",
    "To start, go to the File menu and select Open... to load the textured mesh.\n",
    "\n",
    "**<Ctrl + Left Click>** on any face of the mesh to toggle information about how the texture is mapped onto that specific part of the model. This will show details such as texture coordinates and mapping boundaries.\n",
    "\n",
    "This step is useful for inspecting the accuracy of texture placement and for troubleshooting any issues with the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python visualize_texture_mapping.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
